\documentclass[a4paper,12pt]{article}
\usepackage[cp1251]{inputenc}
\usepackage[english,russian]{babel}
\usepackage[left=2.5cm, right=1.5cm, vmargin=2.5cm]{geometry}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage[dvips]{graphicx} 
\graphicspath{{images/}}
\usepackage{hyperref}
\DeclareMathOperator*{\argmin}{Arg\,min} 

\linespread{1.25}

\begin{document}
	\section*{Abstract}
	.........
	
	\section*{Introduction}
	\label{Introduction}
	
	Time series are widely used in applied science and technology. Time series database management systems are the fastest growing segment in the database industry, which may indicate a growing need for time series forecasting.
	
	Often, time series data is heterogeneous and experiences multiple abrupt changes in structure. These changes are known as changepoints cause the data to be split into segments which can then be modelled separately. Changepoint detection is required in  a number of applications including financial data, bioinformatics, climate data and analysis of speech signals.
	
	As increasingly large data-sets are obtained there is a need for statistical methods for detecting changepoints that are not only accurate but also are computationally efficient.
	
	Many approaches to estimating the number and position of changepoints can be formulated in terms of defining a cost function for a segmentation. They either minimise a penalised version of this cost (the penalised minimisation problem) or minimise the cost under a constraint on the number of changepoints (the constrained minimisation problem).  
	
	In the case where the cost function depends on the data through a sum of segment-specific costs, the minimisation can be done exactly  using dynamic programming. However the exact dynamic programming methods have a cost that increases at least quadratically with the amount of data. 
	
	We can speed up the dynamic programming algorithms by the pruning of the solution space. These algorithms are widely used the techniques "inequality based pruning" and "functional pruning".
	

	
	The focus of this report is on using  the Gaussian cost function and the technique "functional pruning" to solve the penalised minimisation problem  for multivariate changepoint detection. We consider a time series of dimention $p\ge 2$. The case $p = 1$ corresponds to the classical univariate framework, left aside in this study.
	
	We present a new approach to search for changepoints ...
	
	We show that ...
	
	The raport is organised as fallows. We begin in Section 1 by description of the penalised optimisation problems for data segmentation. We then rewiew the existing dynamic programming algorithms and pruning approaches for solving the penalised optimisation problem in Section 2. We introduce the new heuristic algorithms FPOP1, FPOP2, FPOP3 in Section 3 and compare these methods in Section 4......
	\newpage 
	
	\section{ Multiple changepoint detection}
	\label{section1}
		\subsection{Model definition}
		\label{Model definition}
		
		We consider a multivariate time series \textit{x} is a vector of ordered data of dimension \textit{p} and size \textit{n}, that is $x = x_{1:n} = (x^1,.., x^p)_{1:n}\in(\mathbb{R}^p)$ with $x_i^j$ the \textit{j}-th component of the \textit{p}-dimensional point  $x_i\in(\mathbb{R}^p)$ in position \textit{i} in vector \textit{x}. Integer \textit{p} and \textit{n} verify the constraints $n \gg p$ and $p\ge 2$. Within this raport, we use the notation  $x _{s:t} = (x_s,.., x_t)$.
		
		Each data point is generated by \textit{p}-parametric laws from the natural exponential family.Parameters of the laws are constant on each segment (common to all the \textit{p} joint series). We assume an overall independance: components of each data point are independent as well as the successive data points.
		
		The problem of "Multiple changepoint detection" is consisting in finding both the location of changepoints and the parameters associated to each segment and is related to the minimization of cost function linked to the data model in this parametric configuration. We suppose that the cost function $C()$ satisfies the condition (\ref*{eq:cost}) for all $s < t$ and some single observation cost function $\theta \rightarrow \omega(x_i,\theta)$ with observation $x_i\in \mathbb{R}^p$ and parameter $\theta \in \mathbb{R}^p$.
		
		\begin{equation}
			\begin{gathered}
			C(x_{s:t}) = \min_{\theta \in \mathbb{R}^p} \sum_{i=s}^{t}\omega(x_i,\theta).
			\end{gathered}
			\label{eq:cost}
		\end{equation}
	
	 	This cost function is defined as the opposite of log-likelihood of the probability density function. In the case of a natural exponential family (Gaussian, Poisson, Binomial distributions) the cost is piecewise quadratic(Gaussian) or the sum of a linear term with a non-linear convex function (other cases).
	 	
		The segmentation with \textit{k} changepoints is defined by the vector $\tau =(1 = \tau_0,..,\tau_{k+1} = n+1)$. The segments are then given by the sets of indices $(\tau_i, \tau_{i+1}-1), i = 0,..,k$. In order  to find the optimal number of segments, we use a penalised version of the cost by $\beta > 0$. We define the set $ S_n = \{ \tau = ( \tau_0,..,\tau_{k+1} ) \in \mathbb{N}^{k+2} | 1 =\tau_0 <\tau_1 < .. < \tau_k<\tau_{k+1} = n+1 \}  $, then the optimal cost associated to our segmentation problem is:
			
		\begin{equation}
			\begin{gathered}
				Q_n = \min_{\tau \in S_n}\left[ \sum_{i=0}^{k} \{C(x_{\tau_i:\tau_{i+1}-1})+\beta\} \right].
			\end{gathered}
			\label{eq:cost2}
		\end{equation}
		
		Finally, the desired quantity is the argument of the previous minimisation. The optimal segmentation $\tau^*$ is then defined as:
	 
		\begin{equation}
			\begin{gathered}
			\tau^* = \argmin_{\tau \in S_n}\left[ \sum_{i=0}^{k} \{C(x_{\tau_i:\tau_{i+1}-1})+\beta\} \right].
			\end{gathered}
			\label{eq:tau}
		\end{equation}
		
		\subsection{Conditions for pruning}
		\label{Conditions}
		
		 The pruning methods are used for speeding up the dynamic programming algorithms. The pruning methods can be applied under one of two conditions on the segment costs:
		 
		 \begin{itemize}
		 	
		 	\item \textbf{Condition1}: \underline {The functional pruning}
		 	
		 	 The cost function satisfies
		 	 
		 	 \begin{equation}
		 	 	\begin{gathered}
		 	 		C(x_{t+1:s}) = \min_{\theta \in \mathbb{R}^p} \sum_{i=t+1}^{s}\omega(x_i,\theta),
		 	 	\end{gathered}
		 	 	\label{eq:cond1}
		 	 \end{equation}
		 	 
		 	 for some function $\omega()$, with parameter $\theta$.
		 	 
		 	
		 	\item \textbf{Condition2}: \underline {The inequality based pruning}
		 	
		 	There exists a constant \textit{k} such that for $t < s < T$,
		 	
		 	 \begin{equation}
		 		\begin{gathered}
		 			C(x_{t+1:s}) + 	C(x_{s+1:T}) +k \le C(x_{t+1:T}).
		 		\end{gathered}
		 		\label{eq:cond2}
		 	\end{equation}
	 		
		\end{itemize}
	
		\textbf{Note:} Condition1 is a stronger than Condition2. If Condition1 holds then Condition2 also holds with $k = 0$.
		
		 
		
		
	\section{The exact dinamic programming algorithms for solving the penalised minimisation problem}
	\label{section2}
	
	Before mentioning how the pruning can be used to reduce the computational cost we will discuss the initial algorithm, Optimal Partitioning(Jackson et al., 2005). 
	
		\subsection{Optimal Partitioning}
		\label{OP}
		 This method is based on the introduction of the parameter $\theta$, which is the vector of parameters shared by last segment within the data. We define (\ref{eq:cost4}) and obtain the simple recursion(\ref{eq:cost5} for all  $\theta \in\mathbb{R}^p$, with $m_t = \min_\theta Q_t(\theta)$ and the initialisation $Q_0(\theta) = 0$. Each function $Q_t()$ is a piecewise continuous function made of, at most, \textit{t} different functions.
		 
		 \begin{equation}
		 	\begin{gathered}
		 		Q_{t+1} (\theta) = \min_{\tau \in S_{t+1}}\left[ \sum_{i=0}^{k-1} \{C(x_{\tau_i:\tau_{i+1}-1})+\beta\} + \{ \sum_{j=\tau_{k}}^{t+1}  \omega(x_j, \theta)+\beta \} \right].
		 	\end{gathered}
		 	\label{eq:cost4}
		 \end{equation}
		 
		 \begin{equation}
		 	\begin{gathered}
		 		Q_{t+1} (\theta) = \min \{Q_t(\theta),m_t +\beta \} +\omega(x_{t+1}, \theta).
		 	\end{gathered}
		 	\label{eq:cost5}
		 \end{equation}
		 
		 The segmentations themselves can recovered by first taking the arguments which minimise (\ref{eq:cost5}) 
		 
		 \begin{equation}
		 	\begin{gathered}
		 		\tau_t^* = \argmin_{i \in \{1:t-1\}}\{ \min_{\theta \in\mathbb{R}^p }Q_i(\theta)\}.
		 	\end{gathered}
		 	\label{eq:tau2}
		\end{equation}
	 	
	 	As equation (\ref{eq:cost5}) for time steps $t = 2:n+1$ and each time step involves a minimisation over $\tau = 1:t$ the computation actually takes $O(n^2)$ time.
		
		which give the optimal location of the last changepoint in the segmentation of $x_{1:t}$ by $cp(t)$, with $cp(1) = 1$, then the optimal changepoints up to time $t$ can be calculated recursively $cp(t) = (cp(\tau_t^*), \tau_t^*)$.
		\subsection{PELT}
		\label{PELT}
		
		
		\subsection{FPOP}
		\label{FPOP}	
	\section{Heuristic algorithms FPOP1, FPOP2, FPOP3}
	\label{section3}
		\subsection{FPOP1}
		\label{FPOP1}
		\subsection{FPOP2}
		\label{FPOP2}
		\subsection{FPOP3}
		\label{FPOP3}	
		\section{Simulation}
		\section{Dimension 2. FPOP1, FPOP2, FPOP3}
		.......
		\subsection{A multi-dimentional Gaussian cost}
		\label{gaussian cost}
		
		In our study we use the criterio SIC for choice of the penalty $\beta$.
		
		we will restrict our study to the natural exponential familly with a normal distribution.
		
		
		Each data point is generated by \textit{p}-parametric normal distribution law. Parameter $\mu$ is conctant on each segment and parameter  $\sigma$ is conctant for all series (common to all the \textit{p} joint series).
		
		
		Parameter $\mu$ is conctant on each segment and parameter  $\sigma$ is conctant for all series (common to all the \textit{p} joint series).
		
		
		The problem consisting in finding both the location of changepoints and the parameter $\mu$ associate to each segment  is called multiple changepoint detection 
		
		The focus of this report is on the solving the penalised minimisation problem using a dinamic programming approach with the functional pruning.
		
		
			The discrete minimisation of overall cost in (\ref{eq:cost2}) involves the scanning all the $2^{n+1}$ diffrent segmentations.  
\end{document}